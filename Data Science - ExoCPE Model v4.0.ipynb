{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reto: Contribuyentes exonerados\n",
    "Objetivo: Crear un modelo entrenado con un corpus de internet que permita identificar el buen uso del beneficio\n",
    "\n",
    "#### Nota: Para poder correr de forma local el código de PySpark \n",
    "Es necesario configurar las variables de entorno del sistema **\"SPARK_HOME: C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\pyspark\"** y **\"JAVA_HOME: C:\\Program Files\\Java\\jdk1.8.0_144\"**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 Se procede a cargar los datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark import SQLContext\n",
    "sc = SparkContext(\"local\", \"App Name\") \n",
    "spark = SQLContext(sc)#spark antes era sql."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categoria</th>\n",
       "      <th>tipo</th>\n",
       "      <th>fuente</th>\n",
       "      <th>ruc</th>\n",
       "      <th>detalle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>otros</td>\n",
       "      <td>Club</td>\n",
       "      <td>cpe_oct</td>\n",
       "      <td>20144684631</td>\n",
       "      <td>ALQ. STAND  FESTIVAL DE ANIVERSARIO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>otros</td>\n",
       "      <td>Club</td>\n",
       "      <td>cpe_oct</td>\n",
       "      <td>20144684631</td>\n",
       "      <td>ALQ. STAND FESTIVAL DE ANIVERSARIO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>otros</td>\n",
       "      <td>Club</td>\n",
       "      <td>cpe_oct</td>\n",
       "      <td>20144684631</td>\n",
       "      <td>COMISION FESTIVAL DE ANIVERSARIO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>otros</td>\n",
       "      <td>Club</td>\n",
       "      <td>cpe_oct</td>\n",
       "      <td>20144684631</td>\n",
       "      <td>COMISION FESTIVAL DE ANIVERSARIO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>otros</td>\n",
       "      <td>Club</td>\n",
       "      <td>cpe_oct</td>\n",
       "      <td>20144684631</td>\n",
       "      <td>ALQ. STAND FESTIVAL DE ANIVERSARIO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>otros</td>\n",
       "      <td>Club</td>\n",
       "      <td>cpe_oct</td>\n",
       "      <td>20144684631</td>\n",
       "      <td>COMISION FESTIVAL DE ANIVERSARIO 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>otros</td>\n",
       "      <td>Club</td>\n",
       "      <td>cpe_oct</td>\n",
       "      <td>20144684631</td>\n",
       "      <td>ALQ DE STAND - FESTIVAL DE ANIVERSARIO 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>otros</td>\n",
       "      <td>Club</td>\n",
       "      <td>cpe_oct</td>\n",
       "      <td>20100093911</td>\n",
       "      <td>INGRESO NIÇï¿½O CON CONSUMO CHOSICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>otros</td>\n",
       "      <td>Club</td>\n",
       "      <td>cpe_oct</td>\n",
       "      <td>20100093911</td>\n",
       "      <td>INGRESO ADULTO CON CONSUMO CHOSICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>otros</td>\n",
       "      <td>Club</td>\n",
       "      <td>cpe_oct</td>\n",
       "      <td>20100093911</td>\n",
       "      <td>R.C.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>otros</td>\n",
       "      <td>Club</td>\n",
       "      <td>cpe_may</td>\n",
       "      <td>20100093911</td>\n",
       "      <td>ALQUILER DE ESPACIO - 300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>otros</td>\n",
       "      <td>Club</td>\n",
       "      <td>cpe_may</td>\n",
       "      <td>20100093911</td>\n",
       "      <td>SERVICIO DE LUZ - CHOSICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>otros</td>\n",
       "      <td>Club</td>\n",
       "      <td>cpe_may</td>\n",
       "      <td>20100093911</td>\n",
       "      <td>SANDWICH DE CHORIZO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>otros</td>\n",
       "      <td>Club</td>\n",
       "      <td>cpe_may</td>\n",
       "      <td>20100093911</td>\n",
       "      <td>MIXTO SIMPLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>otros</td>\n",
       "      <td>Club</td>\n",
       "      <td>cpe_may</td>\n",
       "      <td>20100093911</td>\n",
       "      <td>JUGO DE PAPAYA - FCHOS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>otros</td>\n",
       "      <td>Club</td>\n",
       "      <td>cpe_may</td>\n",
       "      <td>20100093911</td>\n",
       "      <td>CAFE PASADO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>otros</td>\n",
       "      <td>Club</td>\n",
       "      <td>cpe_oct</td>\n",
       "      <td>20101149421</td>\n",
       "      <td>COTIZACIONES MENSUALES (2017-10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>otros</td>\n",
       "      <td>Club</td>\n",
       "      <td>cpe_may</td>\n",
       "      <td>20101149421</td>\n",
       "      <td>CASILLERO CHICO MES(2018-05)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>otros</td>\n",
       "      <td>Club</td>\n",
       "      <td>cpe_may</td>\n",
       "      <td>20101149421</td>\n",
       "      <td>COTIZACIONES MENSUALES(2018-05)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>otros</td>\n",
       "      <td>Club</td>\n",
       "      <td>cpe_may</td>\n",
       "      <td>20502707770</td>\n",
       "      <td>MERCED CONDUCTIVA MAYO'18 - CUATRIMOTO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   categoria  tipo   fuente          ruc  \\\n",
       "0      otros  Club  cpe_oct  20144684631   \n",
       "1      otros  Club  cpe_oct  20144684631   \n",
       "2      otros  Club  cpe_oct  20144684631   \n",
       "3      otros  Club  cpe_oct  20144684631   \n",
       "4      otros  Club  cpe_oct  20144684631   \n",
       "5      otros  Club  cpe_oct  20144684631   \n",
       "6      otros  Club  cpe_oct  20144684631   \n",
       "7      otros  Club  cpe_oct  20100093911   \n",
       "8      otros  Club  cpe_oct  20100093911   \n",
       "9      otros  Club  cpe_oct  20100093911   \n",
       "10     otros  Club  cpe_may  20100093911   \n",
       "11     otros  Club  cpe_may  20100093911   \n",
       "12     otros  Club  cpe_may  20100093911   \n",
       "13     otros  Club  cpe_may  20100093911   \n",
       "14     otros  Club  cpe_may  20100093911   \n",
       "15     otros  Club  cpe_may  20100093911   \n",
       "16     otros  Club  cpe_oct  20101149421   \n",
       "17     otros  Club  cpe_may  20101149421   \n",
       "18     otros  Club  cpe_may  20101149421   \n",
       "19     otros  Club  cpe_may  20502707770   \n",
       "\n",
       "                                        detalle  \n",
       "0           ALQ. STAND  FESTIVAL DE ANIVERSARIO  \n",
       "1            ALQ. STAND FESTIVAL DE ANIVERSARIO  \n",
       "2              COMISION FESTIVAL DE ANIVERSARIO  \n",
       "3              COMISION FESTIVAL DE ANIVERSARIO  \n",
       "4            ALQ. STAND FESTIVAL DE ANIVERSARIO  \n",
       "5         COMISION FESTIVAL DE ANIVERSARIO 2017  \n",
       "6   ALQ DE STAND - FESTIVAL DE ANIVERSARIO 2017  \n",
       "7          INGRESO NIÇï¿½O CON CONSUMO CHOSICA  \n",
       "8            INGRESO ADULTO CON CONSUMO CHOSICA  \n",
       "9                                          R.C.  \n",
       "10                    ALQUILER DE ESPACIO - 300  \n",
       "11                    SERVICIO DE LUZ - CHOSICA  \n",
       "12                          SANDWICH DE CHORIZO  \n",
       "13                                 MIXTO SIMPLE  \n",
       "14                       JUGO DE PAPAYA - FCHOS  \n",
       "15                                  CAFE PASADO  \n",
       "16             COTIZACIONES MENSUALES (2017-10)  \n",
       "17                 CASILLERO CHICO MES(2018-05)  \n",
       "18              COTIZACIONES MENSUALES(2018-05)  \n",
       "19       MERCED CONDUCTIVA MAYO'18 - CUATRIMOTO  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "directorio = 'D:/usuarios/rlinares/_teen2/3.reto_exonerados_01/01.DB/'\n",
    "archivo1 = 'data_exonerados_modelo.csv'\n",
    "archivo2 = 'Educacion_Bi.csv'\n",
    "fullname1 = os.path.join(directorio,archivo1)\n",
    "fullname2 = os.path.join(directorio,archivo2)\n",
    "data = pd.read_csv(fullname2, sep = ',',encoding='iso-8859-1')\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#spark. antes era sql.\n",
    "data = spark.createDataFrame(data) #Creando un dataframe de PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, StructType, StructField\n",
    "from pyspark.sql.functions import udf, col\n",
    "import unicodedata\n",
    "\n",
    "class Test():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def clearAccents(self, columns):\n",
    "        # Filtramos todas las columnas string de un dataFrame\n",
    "        validCols = [c for (c, t) in filter(lambda t: t[1] == 'string', self.df.dtypes)]\n",
    "        # Si es None o [] se proporciona con parametros de columnas\n",
    "        if (columns == \"*\"): columns = validCols[:]\n",
    "\n",
    "        # Recibe una cadena como argumento\n",
    "        def remove_accents(inputStr):\n",
    "            # Primero, normaliza la cadena:\n",
    "            nfkdStr = unicodedata.normalize('NFKD', inputStr)\n",
    "            # Mantenga los caracteres que no tienen otros caracteres combinados (es decir, acentos caracteres)\n",
    "            withOutAccents = u\"\".join([c for c in nfkdStr if not unicodedata.combining(c)])\n",
    "            return withOutAccents\n",
    "\n",
    "        function = udf(lambda x: remove_accents(x) if x != None else x, StringType())\n",
    "        exprs = [function(col(c)).alias(c) if (c in columns) and (c in validCols) else c for c in self.df.columns]\n",
    "        self.df = self.df.select(*exprs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "def clean_column_row(value):\n",
    "    if type(value)!=str:\n",
    "        return value\n",
    "    value=value.strip()\n",
    "    #value=value.trim()\n",
    "    value=value.lower()\n",
    "    if len(value)==0:\n",
    "        return None\n",
    "    return value\n",
    "    \n",
    "clean_column=F.udf(clean_column_row,\"string\")\n",
    "\n",
    "def depure_string(col):\n",
    "    if type(col)!=str:\n",
    "        return col\n",
    "    return F.regexp_replace(F.regexp_replace(F.regexp_replace(col, \"\\?\", \"\"), \"[^a-z]\", \" \"), \"\\\\s+\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(categoria='otros', tipo='club', fuente='cpe_oct', ruc='20144684631', detalle='alq stand festival de aniversario')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tdata = Test(data)\n",
    "Tdata.clearAccents(columns=\"*\")\n",
    "data = Tdata.df\n",
    "\n",
    "for col in data.columns:\n",
    "    data=data.withColumn(col,clean_column(col))\n",
    "\n",
    "data=data.withColumn('detalle',depure_string('detalle'))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1877\n"
     ]
    }
   ],
   "source": [
    "print(data.count())\n",
    "data = data.select('categoria','tipo','detalle')\n",
    "data = data.dropDuplicates()\n",
    "data = data.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"detalle\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "add_stopwords = [\"http\",\"https\",\"r\",\"f\",\"g\",\"kg\",\"x\", \"n\",\"s\",\"m\",\"00\",\"ml\",\"1\"] \n",
    "stopwordsSpanish = StopWordsRemover.loadDefaultStopWords(\"spanish\")\n",
    "\n",
    "stopwordsRemover1 = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered1\").setStopWords(add_stopwords)\n",
    "stopwordsRemover2 = StopWordsRemover(inputCol=\"filtered1\", outputCol=\"filtered\").setStopWords(stopwordsSpanish)\n",
    "\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover1,stopwordsRemover2])\n",
    "pipelineFit = pipeline.fit(data.filter(data['categoria'] == 'educacion'))\n",
    "data_edu = pipelineFit.transform(data.filter(data['categoria'] == 'educacion'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(word='educacion')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, desc\n",
    "data_word = data_edu.select(explode(data_edu.filtered).alias('word'),'categoria')\n",
    "data_word_edu = data_word.groupBy('categoria','word').count().orderBy(desc('count')).select('word')\n",
    "data_word_edu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_wedu= data_word_edu.toPandas()\n",
    "stopwords_educacion = data_wedu['word'].tolist() #Listado de palabras referidas a educacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\n",
    "# stop words\n",
    "add_stopwords = stopwords_educacion # stop words sobre educacion\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover1, stopwordsRemover2])\n",
    "pipelineFit = pipeline.fit(data.filter(data['categoria'] != 'educacion'))\n",
    "data_sinedu = pipelineFit.transform(data.filter(data['categoria'] != 'educacion'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|categoria|         tipo|             detalle|               words|           filtered1|            filtered|\n",
      "+---------+-------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|educacion|colegios_univ|pe direccin proye...|[pe, direccin, pr...|[pe, direccin, pr...|[pe, direccin, pr...|\n",
      "|educacion|colegios_univ|pension ec ec kin...|[pension, ec, ec,...|[pension, ec, ec,...|[pension, ec, ec,...|\n",
      "|educacion|    educacion|actividades de ev...|[actividades, de,...|[actividades, de,...|[actividades, eva...|\n",
      "|educacion|    educacion|          didactica |         [didactica]|         [didactica]|         [didactica]|\n",
      "|educacion|colegios_univ|     pri pension oct| [pri, pension, oct]| [pri, pension, oct]| [pri, pension, oct]|\n",
      "|educacion|    educacion|constituyen los a...|[constituyen, los...|[constituyen, los...|[constituyen, asp...|\n",
      "|educacion|    educacion|intervencion educ...|[intervencion, ed...|[intervencion, ed...|[intervencion, ed...|\n",
      "|educacion|    educacion|aprendizaje en el...|[aprendizaje, en,...|[aprendizaje, en,...|[aprendizaje, aqu...|\n",
      "|educacion|    educacion|son materiales ed...|[son, materiales,...|[son, materiales,...|[materiales, edit...|\n",
      "|educacion|    educacion|grupos de persona...|[grupos, de, pers...|[grupos, de, pers...|[grupos, personas...|\n",
      "|educacion|colegios_univ|pension elementar...|[pension, element...|[pension, element...|[pension, element...|\n",
      "|educacion|    educacion|intervencion del ...|[intervencion, de...|[intervencion, de...|[intervencion, do...|\n",
      "|educacion|    educacion|            ensenar |           [ensenar]|           [ensenar]|           [ensenar]|\n",
      "|educacion|    educacion|segun la logse ti...|[segun, la, logse...|[segun, la, logse...|[segun, logse, ti...|\n",
      "|educacion|colegios_univ|mora sec multa bi...|[mora, sec, multa...|[mora, sec, multa...|[mora, sec, multa...|\n",
      "|educacion|    educacion|los objetivos son...|[los, objetivos, ...|[los, objetivos, ...|[objetivos, conju...|\n",
      "|educacion|    educacion|  didactica clasica |[didactica, clasica]|[didactica, clasica]|[didactica, clasica]|\n",
      "|educacion|    educacion|    autoaprendizaje |   [autoaprendizaje]|   [autoaprendizaje]|   [autoaprendizaje]|\n",
      "|educacion|    educacion|        metodologia |       [metodologia]|       [metodologia]|       [metodologia]|\n",
      "|educacion|    educacion|niveles de concre...|[niveles, de, con...|[niveles, de, con...|[niveles, concrec...|\n",
      "+---------+-------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data_edu.union(data_sinedu) # se une la data de sin educacion a con educacion\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "import pandas as pd\n",
    "\n",
    "data = data.withColumn('wordCount', f.size(f.col('filtered')))\n",
    "\n",
    "wc = data.toPandas()\n",
    "new_word = []\n",
    "new_tipo = []\n",
    "new_cat = []\n",
    "for i in range(len(wc.filtered)):\n",
    "    if wc.wordCount[i]<=5:\n",
    "        new_word.append(wc.filtered[i])\n",
    "        new_tipo.append(wc.tipo[i])\n",
    "        new_cat.append(wc.categoria[i])\n",
    "    else :\n",
    "        n = round(wc.wordCount[i]/5,0)\n",
    "        for j in range(n.astype(int)-1):\n",
    "            new_word.append(wc.filtered[i][j*5:(j+1)*5])\n",
    "            new_tipo.append(wc.tipo[i])\n",
    "            new_cat.append(wc.categoria[i])\n",
    "        new_word.append(wc.filtered[i][(n-1).astype(int)*5:wc.wordCount[i]])\n",
    "        new_tipo.append(wc.tipo[i])\n",
    "        new_cat.append(wc.categoria[i])\n",
    "newdata = pd.DataFrame({\n",
    "    'categoria': new_cat,\n",
    "    'tipo': new_tipo,\n",
    "    'filtered': new_word\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ndata = spark.createDataFrame(newdata)\n",
    "ndata = ndata.dropDuplicates()\n",
    "ndata = ndata.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------------+\n",
      "|categoria|            filtered|             tipo|\n",
      "+---------+--------------------+-----------------+\n",
      "|educacion|[real, decreto, o...|        educacion|\n",
      "|educacion|[motora, control,...|        educacion|\n",
      "|educacion|[mismo, relacione...|        educacion|\n",
      "|educacion|[proporciona, for...|        educacion|\n",
      "|educacion|[mas, sencillos, ...|        educacion|\n",
      "|educacion|[etc, fomentar, c...|        educacion|\n",
      "|educacion|[diseno, curricul...|        educacion|\n",
      "|    otros|      [canc, evento]|             club|\n",
      "|    otros|[maqueta, pintura...|arte_vinosreserva|\n",
      "|    otros|         [porc, pom]|     restaurantes|\n",
      "|    otros|[lomito, fino, sa...|     restaurantes|\n",
      "|    otros|    [obra, arte, aa]|arte_vinosreserva|\n",
      "|educacion|[pasos, cambiar, ...|        educacion|\n",
      "|educacion|[sistema, educati...|        educacion|\n",
      "|educacion|[propuesta, curri...|        educacion|\n",
      "|educacion|[edad, educacion,...|        educacion|\n",
      "|educacion|[grandes, problem...|        educacion|\n",
      "|educacion|[educacion, forma...|        educacion|\n",
      "|educacion|[poder, ideologia...|        educacion|\n",
      "|educacion|[ampliacion, comp...|        educacion|\n",
      "+---------+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ndata.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, StructType, StructField\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType, IntegerType, StructType, StructField\n",
    "from pyspark.sql.functions import udf, col\n",
    "import unicodedata\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\n",
    "\n",
    "class Test():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def clearAccents(self, columns):\n",
    "        validCols = [c for (c, t) in filter(lambda t: t[1] == 'string', self.df.dtypes)]\n",
    "\n",
    "        if (columns == \"*\"): columns = validCols[:]\n",
    "\n",
    "        def remove_accents(inputStr):\n",
    "            nfkdStr = unicodedata.normalize('NFKD', inputStr)\n",
    "            withOutAccents = u\"\".join([c for c in nfkdStr if not unicodedata.combining(c)])\n",
    "            return withOutAccents\n",
    "\n",
    "        function = udf(lambda x: remove_accents(x) if x != None else x, StringType())\n",
    "        exprs = [function(col(c)).alias(c) if (c in columns) and (c in validCols) else c for c in self.df.columns]\n",
    "        self.df = self.df.select(*exprs)\n",
    "    \n",
    "    \n",
    "#datacpe = spark.sql('''select CONCAT(ruc_emi,'-',cod_tip_cpe,'-',num_serie,'-',num_cpe) as tipo, 'fe' as categoria, detalle  from externo.cpe_det_mayo a inner join preprocesamiento.exonerados_asociacion_cepe_educacion_np86 b on a.ruc_emi=b.ddp_numruc''') \n",
    "archivo3 = 'cpe_.csv'\n",
    "datacpe_pandas = os.path.join(directorio,archivo3)\n",
    "data = pd.read_csv(datacpe_pandas, sep = ',',encoding='iso-8859-1')\n",
    "datacpe = spark.createDataFrame(data)\n",
    "#agregado por ricardo.\n",
    "\n",
    "tdatacpe = Test(datacpe)\n",
    "tdatacpe.clearAccents(columns=\"*\")\n",
    "datacpe = tdatacpe.df\n",
    "for col in datacpe.columns: datacpe=datacpe.withColumn(col,clean_column(col))\n",
    "datacpe=datacpe.withColumn('detalle',depure_string('detalle'))\n",
    "datacpe = datacpe.dropDuplicates()\n",
    "#datacpe = datacpe.na.drop()\n",
    "\n",
    "\n",
    "##stop words\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"detalle\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "add_stopwords = [\"http\",\"https\",\"r\",\"f\",\"g\",\"kg\",\"x\", \"n\",\"s\",\"m\",\"00\",\"ml\",\"1\"] # standard stop words\n",
    "stopwordsSpanish = StopWordsRemover.loadDefaultStopWords(\"spanish\")\n",
    "\n",
    "stopwordsRemover1 = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered1\").setStopWords(add_stopwords)\n",
    "stopwordsRemover2 = StopWordsRemover(inputCol=\"filtered1\", outputCol=\"filtered\").setStopWords(stopwordsSpanish)\n",
    "\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover1, stopwordsRemover2]) \n",
    "pipelineFit = pipeline.fit(datacpe) \n",
    "datacpeset = pipelineFit.transform(datacpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+-----+\n",
      "|categoria|word           |count|\n",
      "+---------+---------------+-----+\n",
      "|fe       |pe             |6325 |\n",
      "|fe       |cctld          |3147 |\n",
      "|fe       |anual          |2705 |\n",
      "|fe       |renovacion     |2161 |\n",
      "|fe       |com            |1384 |\n",
      "|fe       |registro       |1004 |\n",
      "|fe       |curso          |748  |\n",
      "|fe       |induccion      |730  |\n",
      "|fe       |servicio       |676  |\n",
      "|fe       |anos           |479  |\n",
      "|fe       |general        |457  |\n",
      "|fe       |orientacion    |448  |\n",
      "|fe       |ensenanza      |402  |\n",
      "|fe       |cuota          |384  |\n",
      "|fe       |inscripcion    |365  |\n",
      "|fe       |peru           |344  |\n",
      "|fe       |cade           |338  |\n",
      "|fe       |universitario  |327  |\n",
      "|fe       |mayo           |324  |\n",
      "|fe       |may            |297  |\n",
      "|fe       |pension        |267  |\n",
      "|fe       |edicion        |237  |\n",
      "|fe       |lenguaje       |224  |\n",
      "|fe       |ocupacional    |211  |\n",
      "|fe       |provincia      |204  |\n",
      "|fe       |historia       |191  |\n",
      "|fe       |i              |171  |\n",
      "|fe       |lima           |163  |\n",
      "|fe       |unica          |151  |\n",
      "|fe       |antamina       |143  |\n",
      "|fe       |hombre         |141  |\n",
      "|fe       |nuevo          |141  |\n",
      "|fe       |obras          |133  |\n",
      "|fe       |edu            |127  |\n",
      "|fe       |programa       |119  |\n",
      "|fe       |alquiler       |118  |\n",
      "|fe       |jun            |115  |\n",
      "|fe       |org            |96   |\n",
      "|fe       |bimestre       |95   |\n",
      "|fe       |aprendizaje    |93   |\n",
      "|fe       |anexo          |92   |\n",
      "|fe       |completas      |90   |\n",
      "|fe       |libro          |89   |\n",
      "|fe       |psicomotricidad|87   |\n",
      "|fe       |school         |86   |\n",
      "|fe       |seguridad      |84   |\n",
      "|fe       |aos            |82   |\n",
      "|fe       |internos       |80   |\n",
      "|fe       |cursos         |79   |\n",
      "|fe       |nro            |78   |\n",
      "|fe       |impresion      |77   |\n",
      "|fe       |popular        |77   |\n",
      "|fe       |h              |76   |\n",
      "|fe       |b              |74   |\n",
      "|fe       |terapia        |74   |\n",
      "|fe       |motriz         |73   |\n",
      "|fe       |reeducacion    |73   |\n",
      "|fe       |junio          |72   |\n",
      "|fe       |sesiones       |71   |\n",
      "|fe       |taller         |71   |\n",
      "|fe       |movilidad      |68   |\n",
      "|fe       |cm             |67   |\n",
      "|fe       |participacion  |63   |\n",
      "|fe       |formato        |62   |\n",
      "|fe       |grs            |62   |\n",
      "|fe       |d              |62   |\n",
      "|fe       |buenaventura   |60   |\n",
      "|fe       |fecha          |60   |\n",
      "|fe       |gob            |58   |\n",
      "|fe       |cuentos        |57   |\n",
      "|fe       |capacitacion   |57   |\n",
      "|fe       |mate           |56   |\n",
      "|fe       |modificacion   |56   |\n",
      "|fe       |acabado        |55   |\n",
      "|fe       |hudbay         |55   |\n",
      "|fe       |conducta       |54   |\n",
      "|fe       |cita           |54   |\n",
      "|fe       |alquileres     |54   |\n",
      "|fe       |intervencion   |53   |\n",
      "|fe       |adultos        |52   |\n",
      "|fe       |temprana       |51   |\n",
      "|fe       |net            |51   |\n",
      "|fe       |contrato       |50   |\n",
      "|fe       |corrupcion     |50   |\n",
      "|fe       |tahuantinsuyu  |50   |\n",
      "|fe       |papel          |49   |\n",
      "|fe       |ta             |49   |\n",
      "|fe       |c              |48   |\n",
      "|fe       |gestion        |48   |\n",
      "|fe       |andes          |48   |\n",
      "|fe       |ra             |46   |\n",
      "|fe       |interiores     |46   |\n",
      "|fe       |clases         |45   |\n",
      "|fe       |transferencia  |45   |\n",
      "|fe       |quechua        |45   |\n",
      "|fe       |amaru          |44   |\n",
      "|fe       |tupac          |44   |\n",
      "|fe       |cafeterias     |44   |\n",
      "|fe       |pago           |44   |\n",
      "|fe       |mes            |44   |\n",
      "+---------+---------------+-----+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#datacpeset.head()\n",
    "from pyspark.sql.functions import explode, desc\n",
    "\n",
    "data_word = datacpeset.select(explode(datacpeset.filtered).alias('word'),'categoria')\n",
    "data_word.groupBy('categoria','word').count().orderBy(desc('count')).show(100, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----+\n",
      "|word                   |count|\n",
      "+-----------------------+-----+\n",
      "|cctld pe               |3147 |\n",
      "|anual cctld            |2686 |\n",
      "|renovacion anual       |1809 |\n",
      "|com pe                 |1384 |\n",
      "|registro anual         |877  |\n",
      "|anos cctld             |461  |\n",
      "|orientacion induccion  |406  |\n",
      "|induccion general      |406  |\n",
      "|curso orientacion      |406  |\n",
      "|servicio ensenanza     |397  |\n",
      "|renovacion anos        |342  |\n",
      "|inscripcion cade       |329  |\n",
      "|cade universitario     |327  |\n",
      "|ensenanza may          |285  |\n",
      "|curso induccion        |265  |\n",
      "|universitario provincia|204  |\n",
      "|cuota unica            |151  |\n",
      "|antamina hombre        |141  |\n",
      "|hombre nuevo           |141  |\n",
      "|induccion antamina     |141  |\n",
      "|edu pe                 |127  |\n",
      "|registro anos          |119  |\n",
      "|universitario lima     |119  |\n",
      "|ensenanza jun          |112  |\n",
      "|org pe                 |96   |\n",
      "|i bimestre             |95   |\n",
      "|obras completas        |90   |\n",
      "|aos i                  |81   |\n",
      "|cursos internos        |78   |\n",
      "|edicion popular        |77   |\n",
      "|reeducacion motriz     |73   |\n",
      "|school mayo            |70   |\n",
      "|servicio alquiler      |61   |\n",
      "|induccion buenaventura |60   |\n",
      "|gob pe                 |58   |\n",
      "|hudbay anexo           |55   |\n",
      "|induccion hudbay       |55   |\n",
      "|formato cm             |54   |\n",
      "|modificacion conducta  |54   |\n",
      "|cita nro               |54   |\n",
      "|nro fecha              |54   |\n",
      "|historia peru          |53   |\n",
      "|internos adultos       |51   |\n",
      "|intervencion temprana  |51   |\n",
      "|historia tahuantinsuyu |50   |\n",
      "|net pe                 |49   |\n",
      "|corrupcion peru        |48   |\n",
      "|historia corrupcion    |48   |\n",
      "|peru edicion           |48   |\n",
      "|ta edicion             |48   |\n",
      "|alquileres cafeterias  |44   |\n",
      "|tupac amaru            |44   |\n",
      "|pension elementary     |42   |\n",
      "|orientacion general    |42   |\n",
      "|general seguridad      |42   |\n",
      "|elementary school      |42   |\n",
      "|induccion orientacion  |42   |\n",
      "|ra edicion             |41   |\n",
      "|unica induccion        |41   |\n",
      "|peru contemporaneo     |40   |\n",
      "|tahuantinsuyu ra       |40   |\n",
      "|cuota pension          |39   |\n",
      "|adultos regular        |39   |\n",
      "|regular d              |39   |\n",
      "|servicio impresion     |39   |\n",
      "|taller terap           |37   |\n",
      "|mes mayo               |37   |\n",
      "|impresion libro        |36   |\n",
      "|obras escogidas        |36   |\n",
      "|antiguo peru           |36   |\n",
      "|ventas corporativas    |36   |\n",
      "|contrato tercero       |35   |\n",
      "|couche mate            |35   |\n",
      "|contemporaneo ta       |34   |\n",
      "|interiores papel       |34   |\n",
      "|segunda edicion        |33   |\n",
      "|cuentos antiguo        |33   |\n",
      "|mate grs               |33   |\n",
      "|pension empresas       |32   |\n",
      "|rebelion tupac         |32   |\n",
      "|grs caratula           |31   |\n",
      "|transferencia nombre   |31   |\n",
      "|nombre dominio         |31   |\n",
      "|sesiones mayo          |31   |\n",
      "|amaru edicion          |29   |\n",
      "|rendidos don           |28   |\n",
      "|don perdonar           |28   |\n",
      "|ilustre mestiza        |27   |\n",
      "|pizarro ilustre        |27   |\n",
      "|dona francisca         |27   |\n",
      "|francisca pizarro      |27   |\n",
      "|acabado cosido         |27   |\n",
      "|papel couche           |26   |\n",
      "|manejo defensivo       |26   |\n",
      "|participacion torneo   |25   |\n",
      "|lugar campus           |24   |\n",
      "|licencia conducir      |24   |\n",
      "|utec barranco          |24   |\n",
      "|conducir interna       |24   |\n",
      "|duplicado licencia     |24   |\n",
      "+-----------------------+-----+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "bigram = NGram(n=2, inputCol='filtered', outputCol='bigrams')\n",
    "df_bigram = bigram.transform(datacpeset)\n",
    "\n",
    "df_words = df_bigram.select(explode(df_bigram.bigrams).alias('word'))\n",
    "df_words.groupBy('word').count().orderBy(desc('count')).show(100,truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+-----+\n",
      "|word                            |count|\n",
      "+--------------------------------+-----+\n",
      "|anual cctld pe                  |2686 |\n",
      "|renovacion anual cctld          |1809 |\n",
      "|registro anual cctld            |877  |\n",
      "|anos cctld pe                   |461  |\n",
      "|curso orientacion induccion     |406  |\n",
      "|orientacion induccion general   |406  |\n",
      "|renovacion anos cctld           |342  |\n",
      "|inscripcion cade universitario  |323  |\n",
      "|servicio ensenanza may          |285  |\n",
      "|cade universitario provincia    |204  |\n",
      "|antamina hombre nuevo           |141  |\n",
      "|induccion antamina hombre       |141  |\n",
      "|curso induccion antamina        |141  |\n",
      "|registro anos cctld             |119  |\n",
      "|cade universitario lima         |119  |\n",
      "|servicio ensenanza jun          |112  |\n",
      "|aos i bimestre                  |81   |\n",
      "|curso induccion buenaventura    |60   |\n",
      "|induccion hudbay anexo          |55   |\n",
      "|curso induccion hudbay          |55   |\n",
      "|cita nro fecha                  |54   |\n",
      "|cursos internos adultos         |51   |\n",
      "|peru edicion popular            |48   |\n",
      "|historia corrupcion peru        |48   |\n",
      "|corrupcion peru edicion         |48   |\n",
      "|orientacion general seguridad   |42   |\n",
      "|pension elementary school       |42   |\n",
      "|induccion orientacion general   |42   |\n",
      "|unica induccion orientacion     |41   |\n",
      "|cuota unica induccion           |41   |\n",
      "|tahuantinsuyu ra edicion        |40   |\n",
      "|historia tahuantinsuyu ra       |40   |\n",
      "|internos adultos regular        |39   |\n",
      "|adultos regular d               |38   |\n",
      "|elementary school mayo          |35   |\n",
      "|peru contemporaneo ta           |34   |\n",
      "|historia peru contemporaneo     |34   |\n",
      "|contemporaneo ta edicion        |34   |\n",
      "|cuentos antiguo peru            |33   |\n",
      "|rebelion tupac amaru            |32   |\n",
      "|couche mate grs                 |32   |\n",
      "|transferencia nombre dominio    |31   |\n",
      "|tupac amaru edicion             |29   |\n",
      "|amaru edicion popular           |29   |\n",
      "|rendidos don perdonar           |28   |\n",
      "|francisca pizarro ilustre       |27   |\n",
      "|pizarro ilustre mestiza         |27   |\n",
      "|dona francisca pizarro          |27   |\n",
      "|lugar campus central            |24   |\n",
      "|servicio impresion libro        |24   |\n",
      "|participacion torneo adca       |24   |\n",
      "|campus central utec             |24   |\n",
      "|duplicado licencia conducir     |24   |\n",
      "|central utec barranco           |24   |\n",
      "|licencia conducir interna       |24   |\n",
      "|cuota ingreso jul               |23   |\n",
      "|ingreso jul dic                 |23   |\n",
      "|cuota pension bloque            |23   |\n",
      "|pension bloque semestral        |23   |\n",
      "|papel couche mate               |23   |\n",
      "|hombres huarochiri narracion    |22   |\n",
      "|narracion quechua recogida      |22   |\n",
      "|recogida francisco avila        |22   |\n",
      "|quechua recogida francisco      |22   |\n",
      "|dioses hombres huarochiri       |22   |\n",
      "|huarochiri narracion quechua    |22   |\n",
      "|pension middle school           |22   |\n",
      "|clases nacion peru              |21   |\n",
      "|pension ec ec                   |21   |\n",
      "|ec ec kinder                    |21   |\n",
      "|inca yupanqui obras             |21   |\n",
      "|yupanqui obras completas        |21   |\n",
      "|peru nueva edicion              |21   |\n",
      "|obras completas i               |21   |\n",
      "|pachacutec inca yupanqui        |21   |\n",
      "|pension high school             |21   |\n",
      "|nacion peru nueva               |21   |\n",
      "|taller terap habilidades        |20   |\n",
      "|presencial programa desarrollara|20   |\n",
      "|capacitacion supervisores jefes |20   |\n",
      "|modalidad presencial programa   |20   |\n",
      "|terap habilidades sociales      |20   |\n",
      "|sociedad historia peru          |19   |\n",
      "|hrs junio hrs                   |19   |\n",
      "|horario clases mayo             |19   |\n",
      "|hombres cuentos antiguo         |19   |\n",
      "|origen hombres cuentos          |19   |\n",
      "|leading digital modalidad       |19   |\n",
      "|junio horario clases            |19   |\n",
      "|obras completas ix              |19   |\n",
      "|mayo junio horario              |19   |\n",
      "|futbol aos i                    |19   |\n",
      "|desarrollara mayo junio         |19   |\n",
      "|incas obras completas           |19   |\n",
      "|nacion sociedad historia        |19   |\n",
      "|mayo hrs junio                  |19   |\n",
      "|clases mayo hrs                 |19   |\n",
      "|programa desarrollara mayo      |19   |\n",
      "|middle school mayo              |19   |\n",
      "|digital modalidad presencial    |19   |\n",
      "+--------------------------------+-----+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "trigram = NGram(n=3, inputCol='filtered', outputCol='trigrams')\n",
    "df_trigram = trigram.transform(datacpeset)\n",
    "\n",
    "df_words = df_trigram.select(explode(df_trigram.trigrams).alias('word'))\n",
    "df_words.groupBy('word').count().orderBy(desc('count')).show(100,truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "word2Vec = Word2Vec(vectorSize=50, minCount=10, maxIter=50, inputCol='filtered', outputCol='featuresW2V')\n",
    "word2Vec_model = word2Vec.fit(datacpeset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|      word|         similarity|\n",
      "+----------+-------------------+\n",
      "|    school| 0.6379492282867432|\n",
      "| semestral| 0.5397517681121826|\n",
      "|         p| 0.5149340629577637|\n",
      "|        ec| 0.5117691159248352|\n",
      "|    bloque| 0.4828224778175354|\n",
      "|    kinder| 0.4563576281070709|\n",
      "|    codigo| 0.4424918591976166|\n",
      "|elementary|0.42747604846954346|\n",
      "|    manana|0.42707183957099915|\n",
      "|     ciclo|0.42615100741386414|\n",
      "|    middle|0.41408154368400574|\n",
      "| conflicto|0.40872353315353394|\n",
      "|      high|0.39488163590431213|\n",
      "|        er| 0.3848575949668884|\n",
      "|  sistemas| 0.3771640360355377|\n",
      "|     cuota|0.37505635619163513|\n",
      "|   bolivia|0.37245815992355347|\n",
      "| republica|0.37073081731796265|\n",
      "|       hrs| 0.3691304326057434|\n",
      "|   ingreso|0.36626991629600525|\n",
      "+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word2Vec_model.findSynonyms(\"pension\",30).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modificado\n",
    "data_model = datacpeset.select('categoria','tipo','filtered').union(ndata.select('categoria','tipo','filtered'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|categoria|count|\n",
      "+---------+-----+\n",
      "|categoria|    1|\n",
      "|       fe| 8857|\n",
      "|    otros| 1023|\n",
      "|educacion| 1865|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_model.groupBy('categoria').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------------+-------------------------------------------------------------------------------+\n",
      "|categoria|tipo                     |filtered                                                                       |\n",
      "+---------+-------------------------+-------------------------------------------------------------------------------+\n",
      "|fe       |20111451592-01-f003-86911|[renovacion, anual, cctld, pe, planotec, com, pe]                              |\n",
      "|fe       |20147829583-01-f01a-2104 |[lenguaje]                                                                     |\n",
      "|fe       |20147829583-01-f05a-856  |[aprendizaje]                                                                  |\n",
      "|fe       |20147829583-01-f01b-2388 |[ocupacional]                                                                  |\n",
      "|fe       |20111451592-01-f003-84968|[renovacion, anual, cctld, pe, diamanteperu, com, pe]                          |\n",
      "|fe       |20111451592-01-f003-85014|[renovacion, anual, cctld, pe, covar, com, pe]                                 |\n",
      "|fe       |20147829583-01-f04a-842  |[lenguaje]                                                                     |\n",
      "|fe       |20147829583-01-f03a-825  |[aprendizaje]                                                                  |\n",
      "|fe       |20101268943-01-f001-1551 |[distribucion, ingreso, peru]                                                  |\n",
      "|fe       |20117592899-01-ff01-3569 |[credito, educativo, credito, educativo, pago, parcial]                        |\n",
      "|fe       |20117592899-01-ff03-450  |[cuota, unica, operacion, tractor, agricola, utilizacion, accesorios]          |\n",
      "|fe       |20111451592-01-f003-87045|[renovacion, anual, cctld, pe, aguirrenet, com, pe]                            |\n",
      "|fe       |20391062057-01-fl01-1384 |[curso, orientacion, induccion, general]                                       |\n",
      "|fe       |20137254205-01-f140-368  |[inscripcion, cade, universitario, provincia, flor, rocio, murrugarra, casapia]|\n",
      "|fe       |20107259091-01-f001-261  |[kit, escolar, mimate, inicial, anos]                                          |\n",
      "|fe       |20137254205-01-f140-409  |[inscripcion, cade, universitario, provincia, genaro, loa, utani]              |\n",
      "|fe       |20111451592-01-f003-86296|[renovacion, anos, cctld, pe, ecosermrancas, com, pe]                          |\n",
      "|fe       |20111451592-01-f003-86288|[registro, anual, cctld, pe, sharpei, agency, pe]                              |\n",
      "|fe       |20301385464-01-e001-2529 |[pension, hs]                                                                  |\n",
      "|fe       |20301385464-01-e001-2515 |[pension, ec, ec, kinder, mayo]                                                |\n",
      "+---------+-------------------------+-------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_model.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+---+\n",
      "| x1| x2|   x3| x4|\n",
      "+---+---+-----+---+\n",
      "|  1|  a| 23.0|  0|\n",
      "|  3|  B|-23.0|  0|\n",
      "+---+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(1, \"a\", 23.0), (3, \"B\", -23.0)], (\"x1\", \"x2\", \"x3\"))\n",
    "\n",
    "df_with_x4 = df.withColumn(\"x4\", lit(0))\n",
    "df_with_x4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+---+--------------------+\n",
      "| x1| x2|   x3| x4|                  x5|\n",
      "+---+---+-----+---+--------------------+\n",
      "|  1|  a| 23.0|  0| 9.744803446248903E9|\n",
      "|  3|  B|-23.0|  0|1.026187963170189...|\n",
      "+---+---+-----+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import exp\n",
    "\n",
    "df_with_x5 = df_with_x4.withColumn(\"x5\", exp(\"x3\"))\n",
    "df_with_x5.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+---+--------------------+----+\n",
      "| x1| x2|   x3| x4|                  x5|  x6|\n",
      "+---+---+-----+---+--------------------+----+\n",
      "|  1|  a| 23.0|  0| 9.744803446248903E9| foo|\n",
      "|  3|  B|-23.0|  0|1.026187963170189...|null|\n",
      "+---+---+-----+---+--------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lookup = spark.createDataFrame([(1, \"foo\"), (2, \"bar\")], (\"k\", \"v\"))\n",
    "df_with_x6 = (df_with_x5\n",
    "    .join(lookup, df_with_x5.x1 == lookup.k, \"leftouter\")\n",
    "    .drop(\"k\")\n",
    "    .withColumnRenamed(\"v\", \"x6\"))\n",
    "    \n",
    "df_with_x6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "#|    otros| 1023|\n",
    "#|educacion| 1865|\n",
    "data_model_n = data_model.select('categoria','tipo','filtered',f.when(f.array_contains(data_model.filtered, \"cctld\"),'otros').when(data_model.categoria == \"fe\",\"fe\").when(data_model.categoria == \"otros\",\"otros\").when(data_model.categoria == \"educacion\",\"educacion\").otherwise(0).alias('categoria_n'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modificado\n",
    "#data_model_n.write.format(\"parquet\").option(\"encoding\",\"iso-8859-1\").option(\"header\",True).saveAsTable(\"Analytics4.tt_ExoneradosEducacion_Bi_al05\")\n",
    "#data_model_n.groupBy('categoria_n').count().show()\n",
    "data_model_n.toPandas().to_csv('data_model_n_.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modificado\n",
    "#data_model_n = spark.sql('select * from Analytics4.tt_ExoneradosEducacion_Bi_al05')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[countVectors])\n",
    "pipelineFit = pipeline.fit(data_model_n)\n",
    "data_model_n = pipelineFit.transform(data_model_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "dataset_otros_educ = data_model_n.filter(data_model_n.categoria_n != 'fe')\n",
    "dataset_facturas= data_model_n.filter(data_model_n.categoria_n == 'fe')\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol = \"categoria_n\", outputCol = \"label\")\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[label_stringIdx])\n",
    "pipelineFit = pipeline.fit(dataset_otros_educ)\n",
    "dataset_otros_educ = pipelineFit.transform(dataset_otros_educ)\n",
    "\n",
    "pipelineFit = pipeline.fit(dataset_facturas)\n",
    "dataset_facturas = pipelineFit.transform(dataset_facturas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_otros_educ\n",
      "+-----------+-----+-----+\n",
      "|categoria_n|label|count|\n",
      "+-----------+-----+-----+\n",
      "|          0|  2.0|    1|\n",
      "|      otros|  0.0| 4170|\n",
      "|  educacion|  1.0| 1865|\n",
      "+-----------+-----+-----+\n",
      "\n",
      "None\n",
      "dataset_facturas\n",
      "+-----------+-----+-----+\n",
      "|categoria_n|label|count|\n",
      "+-----------+-----+-----+\n",
      "|         fe|  0.0| 5710|\n",
      "+-----------+-----+-----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('dataset_otros_educ')\n",
    "print(dataset_otros_educ.groupBy('categoria_n','label').count().show())\n",
    "print('dataset_facturas')\n",
    "print(dataset_facturas.groupBy('categoria_n','label').count().show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_otros_educ = dataset #.filter(dataset.categoria_n != 'fe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lrModel = lr.fit(dataset_otros_educ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      "3 X 1325 CSRMatrix\n",
      "(0,0) 0.0925\n",
      "(0,1) 0.1855\n",
      "(1,0) -0.0915\n",
      "(1,1) -0.1835\n",
      "Intercept: [2.63967301301,2.18865751406,-4.82833052707]\n"
     ]
    }
   ],
   "source": [
    "print(\"Coefficients: \\n\" + str(lrModel.coefficientMatrix))\n",
    "print(\"Intercept: \" + str(lrModel.interceptVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(dataset_otros_educ)\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(dataset_otros_educ)\n",
    "\n",
    "#labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(dataset_facturas)\n",
    "#featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(dataset_facturas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+\n",
      "|prediction|indexedLabel|            features|\n",
      "+----------+------------+--------------------+\n",
      "|       0.0|         0.0|(1325,[0,1,2,3,4]...|\n",
      "|       0.0|         0.0|(1325,[0,1,2,3],[...|\n",
      "|       0.0|         0.0|(1325,[0,1,2,3,4]...|\n",
      "|       0.0|         0.0|(1325,[0,1,3,9],[...|\n",
      "|       0.0|         0.0|(1325,[0,1,2,3,4]...|\n",
      "|       0.0|         0.0|(1325,[0,1,2,3],[...|\n",
      "|       0.0|         0.0|(1325,[0,1,3,4,9]...|\n",
      "|       0.0|         0.0|(1325,[0,1,2,4,5]...|\n",
      "|       0.0|         0.0|(1325,[0,1,2,3,4]...|\n",
      "|       0.0|         0.0|(1325,[0,1,2,3,4]...|\n",
      "|       0.0|         0.0|(1325,[0,1,2,3,4]...|\n",
      "|       0.0|         0.0|(1325,[0,1,2,3,4]...|\n",
      "|       0.0|         0.0|(1325,[0,1,2,5],[...|\n",
      "|       0.0|         0.0|(1325,[0,1,2,3],[...|\n",
      "|       0.0|         0.0|(1325,[0,1,2,3,4]...|\n",
      "|       0.0|         0.0|(1325,[0,1,2,3],[...|\n",
      "|       0.0|         0.0|(1325,[0,1,2,5,11...|\n",
      "|       0.0|         0.0|(1325,[0,1,2,3],[...|\n",
      "|       0.0|         0.0|(1325,[0,1,2,3],[...|\n",
      "|       0.0|         0.0|(1325,[0,1,2,5],[...|\n",
      "|       0.0|         0.0|(1325,[0,1,2,3],[...|\n",
      "|       0.0|         0.0|(1325,[0,1,2,5],[...|\n",
      "|       0.0|         0.0|(1325,[0,1,3,4,9]...|\n",
      "|       0.0|         0.0|(1325,[0,1,2,4,5]...|\n",
      "|       0.0|         0.0|(1325,[0,1,2,3],[...|\n",
      "|       0.0|         0.0|(1325,[0,1,2,3,4]...|\n",
      "|       0.0|         0.0|(1325,[0,1,2,3],[...|\n",
      "|       0.0|         0.0|(1325,[0,1,2,3,4]...|\n",
      "|       0.0|         0.0|(1325,[0,1,3,9,40...|\n",
      "|       0.0|         0.0|(1325,[0,1,2,3,4]...|\n",
      "|       0.0|         0.0|(1325,[0,1,2,3],[...|\n",
      "|       0.0|         0.0|(1325,[0,1,2,3],[...|\n",
      "|       0.0|         0.0|(1325,[0,1,2,3,35...|\n",
      "|       0.0|         0.0|(1325,[0,1,2,3,4]...|\n",
      "|       0.0|         0.0|(1325,[0,1,2,3,4,...|\n",
      "|       0.0|         0.0|(1325,[0,1,2,3],[...|\n",
      "|       0.0|         0.0|(1325,[0,1,2,3,4]...|\n",
      "|       0.0|         0.0|(1325,[0,1,2,3,35...|\n",
      "|       0.0|         0.0|(1325,[0,1,2,3],[...|\n",
      "|       0.0|         0.0|(1325,[0,1,2,3],[...|\n",
      "+----------+------------+--------------------+\n",
      "only showing top 40 rows\n",
      "\n",
      "Test Error = 0.149543 \n",
      "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_4c99b364be39dcc7e9c8) of depth 5 with 11 nodes\n"
     ]
    }
   ],
   "source": [
    "(trainingData, testData) = dataset_otros_educ.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(40)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
    "\n",
    "treeModel = model.stages[2]\n",
    "# summary only\n",
    "print(treeModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+\n",
      "|prediction|label|count|\n",
      "+----------+-----+-----+\n",
      "|       1.0|  1.0|  528|\n",
      "|       1.0|  0.0|  262|\n",
      "|       0.0|  0.0|  962|\n",
      "+----------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.groupBy('prediction','label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_factura = data_model_n.filter(data_model_n.categoria_n == 'fe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#modificado\n",
    "labelIndexer = StringIndexer(inputCol=\"categoria_n\", outputCol=\"indexedLabel\",handleInvalid=\"skip\")\n",
    "\n",
    "pipeline = Pipeline(stages=[labelIndexer])\n",
    "pipelineFit = pipeline.fit(data_factura)\n",
    "data_factura = pipelineFit.transform(data_factura)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(data_factura)\n",
    "\n",
    "# Select example rows to display.\n",
    "#predictions.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[categoria: string, tipo: string, filtered: array<string>, categoria_n: string, features: vector, indexedLabel: double, indexedFeatures: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o4111.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 664.0 failed 1 times, most recent failure: Lost task 0.0 in stage 664.0 (TID 29079, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$11: (vector) => vector)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: VectorIndexer encountered invalid value 1.0 on feature index 23. To handle or skip invalid value, try setting VectorIndexer.handleInvalid.\r\n\tat org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$10.apply(VectorIndexer.scala:400)\r\n\tat org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$10.apply(VectorIndexer.scala:356)\r\n\tat org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$11.apply(VectorIndexer.scala:431)\r\n\tat org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$11.apply(VectorIndexer.scala:431)\r\n\t... 19 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3272)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3253)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3252)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2484)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2698)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\r\n\tat sun.reflect.GeneratedMethodAccessor200.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$11: (vector) => vector)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: org.apache.spark.SparkException: VectorIndexer encountered invalid value 1.0 on feature index 23. To handle or skip invalid value, try setting VectorIndexer.handleInvalid.\r\n\tat org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$10.apply(VectorIndexer.scala:400)\r\n\tat org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$10.apply(VectorIndexer.scala:356)\r\n\tat org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$11.apply(VectorIndexer.scala:431)\r\n\tat org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$11.apply(VectorIndexer.scala:431)\r\n\t... 19 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-163-dd7a8263445b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    348\u001b[0m         \"\"\"\n\u001b[0;32m    349\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\py4j-0.10.6-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1160\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1162\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\py4j-0.10.6-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    319\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    321\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o4111.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 664.0 failed 1 times, most recent failure: Lost task 0.0 in stage 664.0 (TID 29079, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$11: (vector) => vector)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: VectorIndexer encountered invalid value 1.0 on feature index 23. To handle or skip invalid value, try setting VectorIndexer.handleInvalid.\r\n\tat org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$10.apply(VectorIndexer.scala:400)\r\n\tat org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$10.apply(VectorIndexer.scala:356)\r\n\tat org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$11.apply(VectorIndexer.scala:431)\r\n\tat org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$11.apply(VectorIndexer.scala:431)\r\n\t... 19 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3272)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3253)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3252)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2484)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2698)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\r\n\tat sun.reflect.GeneratedMethodAccessor200.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$11: (vector) => vector)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: org.apache.spark.SparkException: VectorIndexer encountered invalid value 1.0 on feature index 23. To handle or skip invalid value, try setting VectorIndexer.handleInvalid.\r\n\tat org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$10.apply(VectorIndexer.scala:400)\r\n\tat org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$10.apply(VectorIndexer.scala:356)\r\n\tat org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$11.apply(VectorIndexer.scala:431)\r\n\tat org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$11.apply(VectorIndexer.scala:431)\r\n\t... 19 more\r\n"
     ]
    }
   ],
   "source": [
    "predictions.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+----------+-----------+-----------------+------------+-----------------+------------------+--------------------+----------+\n",
      "|categoria|                tipo|  filtered|categoria_n|         features|indexedLabel|  indexedFeatures|     rawPrediction|         probability|prediction|\n",
      "+---------+--------------------+----------+-----------+-----------------+------------+-----------------+------------------+--------------------+----------+\n",
      "|       fe|20147829583-01-f0...|[lenguaje]|         fe|(1325,[22],[1.0])|         0.0|(1325,[22],[1.0])|[634.0,1337.0,1.0]|[0.32150101419878...|       1.0|\n",
      "+---------+--------------------+----------+-----------+-----------------+------------+-----------------+------------------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3732.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 657.0 failed 1 times, most recent failure: Lost task 0.0 in stage 657.0 (TID 28274, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$11: (vector) => vector)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: VectorIndexer encountered invalid value 1.0 on feature index 23. To handle or skip invalid value, try setting VectorIndexer.handleInvalid.\r\n\tat org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$10.apply(VectorIndexer.scala:400)\r\n\tat org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$10.apply(VectorIndexer.scala:356)\r\n\tat org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$11.apply(VectorIndexer.scala:431)\r\n\tat org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$11.apply(VectorIndexer.scala:431)\r\n\t... 19 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply$mcI$sp(Dataset.scala:3195)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3192)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3192)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:3225)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3192)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$11: (vector) => vector)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: org.apache.spark.SparkException: VectorIndexer encountered invalid value 1.0 on feature index 23. To handle or skip invalid value, try setting VectorIndexer.handleInvalid.\r\n\tat org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$10.apply(VectorIndexer.scala:400)\r\n\tat org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$10.apply(VectorIndexer.scala:356)\r\n\tat org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$11.apply(VectorIndexer.scala:431)\r\n\tat org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$11.apply(VectorIndexer.scala:431)\r\n\t... 19 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-148-14a90889a87c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#predictions.toPandas().to_csv('predictions_.csv')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mhead\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1132\u001b[0m             \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrs\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1134\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m    502\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Alice'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Bob'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m         \"\"\"\n\u001b[1;32m--> 504\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    464\u001b[0m         \"\"\"\n\u001b[0;32m    465\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    467\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\py4j-0.10.6-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1160\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1162\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\py4j-0.10.6-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    319\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    321\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o3732.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 657.0 failed 1 times, most recent failure: Lost task 0.0 in stage 657.0 (TID 28274, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$11: (vector) => vector)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: VectorIndexer encountered invalid value 1.0 on feature index 23. To handle or skip invalid value, try setting VectorIndexer.handleInvalid.\r\n\tat org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$10.apply(VectorIndexer.scala:400)\r\n\tat org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$10.apply(VectorIndexer.scala:356)\r\n\tat org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$11.apply(VectorIndexer.scala:431)\r\n\tat org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$11.apply(VectorIndexer.scala:431)\r\n\t... 19 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply$mcI$sp(Dataset.scala:3195)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3192)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3192)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:3225)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3192)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$11: (vector) => vector)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: org.apache.spark.SparkException: VectorIndexer encountered invalid value 1.0 on feature index 23. To handle or skip invalid value, try setting VectorIndexer.handleInvalid.\r\n\tat org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$10.apply(VectorIndexer.scala:400)\r\n\tat org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$10.apply(VectorIndexer.scala:356)\r\n\tat org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$11.apply(VectorIndexer.scala:431)\r\n\tat org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$11.apply(VectorIndexer.scala:431)\r\n\t... 19 more\r\n"
     ]
    }
   ],
   "source": [
    "#predictions.toPandas().to_csv('predictions_.csv')\n",
    "type(predictions)\n",
    "predictions.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SQLContext' object has no attribute 'version'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-164-e4c2341a7d22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'SQLContext' object has no attribute 'version'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
